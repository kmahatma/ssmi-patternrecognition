{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlZ19CjFVF/tvoRRmjXagg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmahatma/ssmi-patternrecognition/blob/main/w06_03_nb_fake_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the default working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZtyAp4l1M-F",
        "outputId": "1a026886-efdc-483e-d54c-1e50db0b58d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install & Import Required Libraries"
      ],
      "metadata": {
        "id": "P83KdJObAKAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn nltk\n"
      ],
      "metadata": {
        "id": "ixrZr79QAADA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "puhhVotFAFNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset"
      ],
      "metadata": {
        "id": "pDTlnV0xAO4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "\n",
        "# Add labels: 1 = Fake, 0 = Real\n",
        "df_fake['label'] = 1\n",
        "df_real['label'] = 0\n",
        "\n",
        "# Combine both datasets\n",
        "df = pd.concat([df_fake, df_real], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Check dataset structure\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "BV5MvlnFARVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Preprocess the Text Data\n",
        "Since BernoulliNB works with binary features, we preprocess the text:\n",
        "\n",
        "*  Remove special characters and convert text to lowercase.\n",
        "*  Remove stopwords\n",
        "*   Use CountVectorizer (binary=True) to convert words into a presence/absence matrix."
      ],
      "metadata": {
        "id": "3FlXbwbnAU4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text'] = df['title'] + \" \" + df['text']  # Combine title & content\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords function\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Apply stopword removal\n",
        "df['text'] = df['text'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "XL0Amn3tAYXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Convert Text into Binary Features**\n",
        "Using CountVectorizer (binary=True) ensures that each word is represented as 1 if present, 0 if absent."
      ],
      "metadata": {
        "id": "KKY_ENvKAc0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text into binary feature matrix\n",
        "vectorizer = CountVectorizer(binary=True, stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Target variable (labels)\n",
        "y = df['label']\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ouy-m7CiAera"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train the Bernoulli Naïve Bayes Model"
      ],
      "metadata": {
        "id": "dSbMy4WPAhZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Bernoulli Naïve Bayes Classifier\n",
        "model = BernoulliNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "DmyhqJSeAmZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate the Model"
      ],
      "metadata": {
        "id": "F8uLmJ9qBHhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n"
      ],
      "metadata": {
        "id": "kdY12f5ZBJ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Test with a Custom Fake News Example"
      ],
      "metadata": {
        "id": "l__xjASGBLfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fake_news(text):\n",
        "    cleaned_text = clean_text(text)\n",
        "    cleaned_text = remove_stopwords(cleaned_text)\n",
        "    text_vectorized = vectorizer.transform([cleaned_text])\n",
        "    prediction = model.predict(text_vectorized)[0]\n",
        "    return \"FAKE NEWS\" if prediction == 1 else \"REAL NEWS\"\n",
        "\n",
        "# Example 1\n",
        "news1 = \"Breaking: Government Announces New COVID-19 Restrictions!\"\n",
        "print(f\"News: {news1} → {predict_fake_news(news1)}\")\n",
        "\n",
        "# Example 2\n",
        "news2 = \"Shocking: Scientists Discover Secret Cure for Cancer!\"\n",
        "print(f\"News: {news2} → {predict_fake_news(news2)}\")\n"
      ],
      "metadata": {
        "id": "dmK4h6RQBOfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}